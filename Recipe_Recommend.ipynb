{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3103832e-d1b2-43a1-b92b-81074fd5b58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK data...\n",
      "NLTK data downloaded.\n",
      "Starting the model building process...\n",
      "Loading dataset: Recipe_dataset.csv\n",
      "Processing ingredients for all recipes... (This might take a few minutes)\n",
      "Building TF-IDF matrix...\n",
      "Saving objects to .pkl files...\n",
      "\n",
      "Process complete! The following files have been created:\n",
      "- tfidf_vectorizer.pkl\n",
      "- tfidf_matrix.pkl\n",
      "- recipe_df.pkl\n",
      "\n",
      "You are now ready to run the Streamlit app.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Import Necessary Libraries ---\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import nltk\n",
    "import pickle # Used to save our Python objects\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# --- 2. Download NLTK Data (if not already downloaded) ---\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK data...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    print(\"NLTK data downloaded.\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# --- 3. Define the Preprocessing Function (exactly as before) ---\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "remove_words = set([\n",
    "    'cup', 'cups', 'teaspoon', 'teaspoons', 'tablespoon', 'tablespoons', 'g', 'kg',\n",
    "    'oz', 'ounce', 'ounces', 'pound', 'pounds', 'lb', 'lbs', 'clove', 'cloves',\n",
    "    'fresh', 'chopped', 'sliced', 'diced', 'minced', 'large', 'small', 'medium',\n",
    "    'package', 'can', 'jar', 'bottle', 'pinch', 'dash', 'to', 'taste', 'or'\n",
    "])\n",
    "\n",
    "def clean_and_process_ingredients(ingredient_list_str):\n",
    "    try:\n",
    "        ingredient_list = ast.literal_eval(ingredient_list_str)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return \"\"\n",
    "\n",
    "    cleaned_ingredients = []\n",
    "    for ingredient_text in ingredient_list:\n",
    "        ingredient_text = ingredient_text.lower()\n",
    "        ingredient_text = re.sub(r'\\d+/\\d+|\\d+', '', ingredient_text)\n",
    "        ingredient_text = re.sub(r'[^\\w\\s]', '', ingredient_text)\n",
    "        tokens = word_tokenize(ingredient_text)\n",
    "        for word in tokens:\n",
    "            lemmatized_word = lemmatizer.lemmatize(word)\n",
    "            if lemmatized_word and lemmatized_word not in stop_words and lemmatized_word not in remove_words:\n",
    "                cleaned_ingredients.append(lemmatized_word)\n",
    "    return ' '.join(cleaned_ingredients)\n",
    "\n",
    "# --- 4. Load, Process, and Save the Data ---\n",
    "print(\"Starting the model building process...\")\n",
    "\n",
    "try:\n",
    "    # Load the dataset\n",
    "    print(\"Loading dataset: Recipe_dataset.csv\")\n",
    "    df = pd.read_csv('Recipe_dataset.csv')\n",
    "\n",
    "    # Clean the ingredients\n",
    "    print(\"Processing ingredients for all recipes... (This might take a few minutes)\")\n",
    "    df['cleaned_ingredients'] = df['ingredients'].apply(clean_and_process_ingredients)\n",
    "\n",
    "    # Create and fit the TF-IDF Vectorizer\n",
    "    print(\"Building TF-IDF matrix...\")\n",
    "    tfidf = TfidfVectorizer(stop_words='english', min_df=2)\n",
    "    tfidf_matrix = tfidf.fit_transform(df['cleaned_ingredients'])\n",
    "\n",
    "    # --- 5. Save the Necessary Objects to .pkl Files ---\n",
    "    print(\"Saving objects to .pkl files...\")\n",
    "    \n",
    "    # Save the TF-IDF Vectorizer\n",
    "    with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "        pickle.dump(tfidf, f)\n",
    "        \n",
    "    # Save the TF-IDF Matrix\n",
    "    with open('tfidf_matrix.pkl', 'wb') as f:\n",
    "        pickle.dump(tfidf_matrix, f)\n",
    "        \n",
    "    # Save the DataFrame (we need it to look up recipe details)\n",
    "    # We'll drop the cleaned column to save space\n",
    "    df_to_save = df.drop(columns=['cleaned_ingredients'])\n",
    "    with open('recipe_df.pkl', 'wb') as f:\n",
    "        pickle.dump(df_to_save, f)\n",
    "\n",
    "    print(\"\\nProcess complete! The following files have been created:\")\n",
    "    print(\"- tfidf_vectorizer.pkl\")\n",
    "    print(\"- tfidf_matrix.pkl\")\n",
    "    print(\"- recipe_df.pkl\")\n",
    "    print(\"\\nYou are now ready to run the Streamlit app.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: '1_Recipe_csv.csv' not found. Please place it in the same directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d8e901-ac0d-420c-80ae-4e7f677f1ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
